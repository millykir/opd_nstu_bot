import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import os
import warnings
import numpy as np

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
LOG_FILE = 'chat_qa_log.txt'
OUTPUT_DIR = 'analytics_ultra_report'

# –ß–ï–†–ù–´–ô –°–ü–ò–°–û–ö (–ú–∏—Ö–∞–∏–ª, –ó–∞—Ö–∞—Ä, –ï–≥–æ—Ä)
EXCLUDED_IDS = {'6753772275', '814358254', '1270577551'}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç–∏–ª—è
warnings.simplefilter(action='ignore')
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_context("talk")

STOPWORDS = {
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É', '–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–±–æ—Ç', '–ø–æ–¥—Å–∫–∞–∂–∏', '—Å–∫–∞–∂–∏'
}

def parse_log_file(filepath):
    print(f"üîÆ –ó–∞–ø—É—Å–∫ –Ω–µ–π—Ä–æ-–∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤: {filepath}...")
    data = []
    current_entry = {}
    
    patterns = {
        'user_id': re.compile(r'^UserID:\s*(.*)'),
        'timestamp': re.compile(r'^–í—Ä–µ–º—è —Å–æ–æ–±—â–µ–Ω–∏—è:\s*(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})'),
        'latency': re.compile(r'^–ó–∞–¥–µ—Ä–∂–∫–∞ \(—Å–µ–∫\):\s*([\d\.]+)'),
    }
    
    if not os.path.exists(filepath): return pd.DataFrame()

    with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
        lines = f.readlines()

    q_buf, a_buf = [], []
    
    for line in lines:
        line = line.strip()
        if line.startswith('----------'):
            if current_entry and ('question' in current_entry or q_buf):
                current_entry['question'] = " ".join(q_buf).strip()
                current_entry['answer'] = " ".join(a_buf).strip()
                if not current_entry['question']: current_entry['question'] = "<Empty>"
                data.append(current_entry)
            current_entry = {}
            q_buf, a_buf = [], []
            continue

        if line.startswith('Q:'):
            q_buf.append(line[2:].strip())
        elif line.startswith('A:'):
            a_buf.append(line[2:].strip())
        else:
            for k, p in patterns.items():
                m = p.match(line)
                if m:
                    val = m.group(1).strip()
                    if k == 'latency': 
                        try: current_entry[k] = float(val)
                        except: pass
                    elif k == 'timestamp':
                        try: current_entry['datetime'] = datetime.strptime(val, '%Y-%m-%d %H:%M:%S')
                        except: pass
                    else: current_entry[k] = val

    if current_entry and q_buf:
        current_entry['question'] = " ".join(q_buf).strip()
        current_entry['answer'] = " ".join(a_buf).strip()
        data.append(current_entry)

    df = pd.DataFrame(data)
    
    # --- –¢–û–¢–ê–õ–¨–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ---
    if 'user_id' in df.columns:
        before = len(df)
        df = df[~df['user_id'].astype(str).isin(EXCLUDED_IDS)]
        print(f"üöÆ –ò—Å–∫–ª—é—á–µ–Ω–æ {before - len(df)} –∑–∞–ø–∏—Å–µ–π (–¢–µ—Å—Ç—ã/–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏).")
        
    return df

def get_ngrams(text_series, n=2, top_k=10):
    ngrams_list = []
    valid_texts = text_series.dropna().astype(str)
    for text in valid_texts:
        if text == "<Empty>": continue
        text = re.sub(r'[^\w\s]', '', text.lower())
        words = [w for w in text.split() if w not in STOPWORDS and len(w) > 3]
        if len(words) >= n:
            ngrams_list.extend(zip(*[words[i:] for i in range(n)]))
    return Counter(ngrams_list).most_common(top_k)

def generate_report():
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
    
    df = parse_log_file(LOG_FILE)
    if df.empty:
        print("‚ùå –î–∞–Ω–Ω—ã—Ö –Ω–µ—Ç.")
        return

    df_time = df.dropna(subset=['datetime']).copy()
    print(f"üî¨ –ê–Ω–∞–ª–∏–∑ {len(df_time)} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–µ—Å—Å–∏–π...")

    # 1. RAG HEALTH (DONUT)
    plt.figure(figsize=(8, 8))
    fail_phrases = ['–∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é', '–Ω–µ –Ω–∞—à–µ–ª', '–Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏', '–æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫', '–ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å']
    is_fail = df['answer'].fillna('').str.lower().apply(lambda x: any(p in x for p in fail_phrases))
    success_rate = (1 - is_fail.mean()) * 100
    
    colors = ['#00b894', '#d63031']
    plt.pie([success_rate, 100-success_rate], labels=['–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç', '–ù–µ—Ç –≤ –±–∞–∑–µ'], 
            autopct='%1.1f%%', startangle=90, colors=colors, wedgeprops={'width': 0.4})
    plt.title('RAG System Health Index', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{OUTPUT_DIR}/1_rag_donut_pro.png', dpi=300)
    print("‚úÖ RAG Donut Chart –≥–æ—Ç–æ–≤.")

    # 2. ACTIVITY WAVE
    if not df_time.empty:
        plt.figure(figsize=(12, 6))
        df_time['date'] = df_time['datetime'].dt.date
        daily = df_time['date'].value_counts().sort_index()
        
        plt.fill_between(daily.index, daily.values, color='#0984e3', alpha=0.3)
        plt.plot(daily.index, daily.values, color='#0984e3', linewidth=2)
        plt.title('–í–æ–ª–Ω—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ (Timeline)', fontsize=16)
        plt.grid(True, linestyle=':', alpha=0.6)
        plt.tight_layout()
        plt.savefig(f'{OUTPUT_DIR}/2_timeline_wave.png', dpi=300)
        print("‚úÖ Timeline Wave –≥–æ—Ç–æ–≤.")

    # 3. LATENCY VIOLIN
    if 'latency' in df_time.columns:
        plt.figure(figsize=(10, 6))
        clean_latency = df_time[df_time['latency'] < 40]['latency']
        sns.violinplot(x=clean_latency, color='#6c5ce7')
        plt.title('–î–ù–ö –°–∫–æ—Ä–æ—Å—Ç–∏: –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∑–∞–¥–µ—Ä–∂–∫–∏ (Violin)', fontsize=16)
        plt.xlabel('–°–µ–∫—É–Ω–¥—ã')
        plt.tight_layout()
        plt.savefig(f'{OUTPUT_DIR}/3_latency_violin.png', dpi=300)
        print("‚úÖ Latency Violin –≥–æ—Ç–æ–≤.")

    # 4. ENGAGEMENT HEXBIN
    if not df_time.empty and 'user_id' in df_time.columns:
        user_stats = df_time.groupby('user_id').agg(
            msg_count=('question', 'count'),
            first_msg=('datetime', 'min'),
            last_msg=('datetime', 'max')
        )
        user_stats['lifetime_days'] = (user_stats['last_msg'] - user_stats['first_msg']).dt.total_seconds() / 86400
        
        plt.figure(figsize=(10, 6))
        plt.hexbin(user_stats['lifetime_days'], user_stats['msg_count'], gridsize=15, cmap='Purples', mincnt=1)
        plt.colorbar(label='–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å—Ç—É–¥–µ–Ω—Ç–æ–≤')
        plt.title('–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –ñ–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª vs –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', fontsize=16)
        plt.xlabel('–î–Ω–µ–π —Å –±–æ—Ç–æ–º')
        plt.ylabel('–°–æ–æ–±—â–µ–Ω–∏–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ')
        plt.savefig(f'{OUTPUT_DIR}/4_engagement_hex.png', dpi=300)
        print("‚úÖ Engagement Hexbin –≥–æ—Ç–æ–≤.")

    # 5. SEMANTIC CORE
    bigrams = get_ngrams(df['question'], n=2, top_k=10)
    if bigrams:
        plt.figure(figsize=(10, 8))
        phrases, counts = zip(*bigrams)
        phrases = [" ".join(p).upper() for p in phrases]
        
        sns.barplot(x=list(counts), y=list(phrases), palette='Spectral')
        plt.title('–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ (–ë–æ–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤)', fontsize=16)
        plt.tight_layout()
        plt.savefig(f'{OUTPUT_DIR}/5_semantics_pro.png', dpi=300)
        print("‚úÖ Semantic Bars –≥–æ—Ç–æ–≤.")

    # 6. SCALABILITY
    if not df_time.empty:
        df_time['hour_bucket'] = df_time['datetime'].dt.floor('h')
        load = df_time.groupby('hour_bucket').agg(Load=('question', 'count'), Latency=('latency', 'mean')).dropna()
        
        plt.figure(figsize=(10, 6))
        sns.regplot(data=load, x='Load', y='Latency', scatter_kws={'s':60, 'alpha':0.6}, line_kws={'color':'red'})
        plt.title('–°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç: –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã', fontsize=16)
        plt.savefig(f'{OUTPUT_DIR}/6_scalability.png', dpi=300)
        print("‚úÖ Scalability Test –≥–æ—Ç–æ–≤.")

    df.to_excel(f'{OUTPUT_DIR}/ULTIMATE_DATA.xlsx', index=False)
    print(f"\nüèÜ –ì–ï–ù–ò–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –°–û–ó–î–ê–ù: {OUTPUT_DIR}")

if __name__ == "__main__":
    generate_report()